{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVFr_WZ4o2Xr",
        "outputId": "e3c7f044-5596-40a1-a15f-d654896fa790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2FeatureExtractor, WavLMModel\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "e-Zg0Eh0pIwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "TTfAsesNpLLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"**pre-trained Model**\"\"\"\n",
        "\n",
        "# Load pre-trained WavLM Base Plus model and feature extractor\n",
        "model_name = \"microsoft/wavlm-base-plus\"\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "model = WavLMModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Function to load and preprocess audio\n",
        "def load_audio(file_path, target_sr=16000):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    if sample_rate != target_sr:\n",
        "        waveform = torchaudio.transforms.Resample(sample_rate, target_sr)(waveform)\n",
        "    return waveform.squeeze(0)  # Remove channel dimension if mono\n",
        "\n",
        "# Function to extract embeddings\n",
        "def extract_embedding(audio_path):\n",
        "    waveform = load_audio(audio_path)\n",
        "    # Process audio with feature extractor\n",
        "    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    input_values = inputs[\"input_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_values)\n",
        "        # Use the mean of the last hidden state as the embedding\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    return embedding\n",
        "\n",
        "# Cosine similarity function\n",
        "cosine_similarity = nn.CosineSimilarity(dim=0, eps=1e-6)"
      ],
      "metadata": {
        "id": "Kngadc1WpNaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "voxceleb_root = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/wav\"\n",
        "trial_file = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/VoxCeleb1-cleaned.txt\""
      ],
      "metadata": {
        "id": "itdvlpV8pQn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trial pairs\n",
        "trials = []\n",
        "with open(trial_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        label, file1, file2 = line.strip().split()\n",
        "        trials.append((int(label), file1, file2))\n",
        "\n",
        "# Dictionary to cache embeddings\n",
        "embedding_cache = {}\n",
        "\n",
        "# Compute similarity scores\n",
        "scores = []\n",
        "labels = []\n",
        "for label, file1, file2 in tqdm(trials[:1000]):\n",
        "    file1_path = os.path.join(voxceleb_root, file1)\n",
        "    file2_path = os.path.join(voxceleb_root, file2)\n",
        "\n",
        "    # Verify file existence\n",
        "    if not os.path.exists(file1_path) or not os.path.exists(file2_path):\n",
        "        print(f\"Skipping missing file: {file1_path} or {file2_path}\")\n",
        "        continue\n",
        "\n",
        "    # Get embeddings (cache to avoid recomputation)\n",
        "    if file1_path not in embedding_cache:\n",
        "        embedding_cache[file1_path] = extract_embedding(file1_path)\n",
        "    if file2_path not in embedding_cache:\n",
        "        embedding_cache[file2_path] = extract_embedding(file2_path)\n",
        "\n",
        "    emb1 = torch.from_numpy(embedding_cache[file1_path]).to(device)\n",
        "    emb2 = torch.from_numpy(embedding_cache[file2_path]).to(device)\n",
        "\n",
        "    score = cosine_similarity(emb1, emb2).item()\n",
        "    scores.append(score)\n",
        "    labels.append(label)"
      ],
      "metadata": {
        "id": "MVSmpHj1pSqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric 1: EER (in %)\n",
        "def compute_eer(labels, scores):\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
        "    fnr = 1 - tpr\n",
        "    eer_threshold = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "    eer = interp1d(fpr, fnr)(eer_threshold)\n",
        "    return eer * 100\n",
        "\n",
        "eer = compute_eer(labels, scores)\n",
        "print(f\"Equal Error Rate (EER): {eer:.2f}%\")\n"
      ],
      "metadata": {
        "id": "XyjJRasWpV-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric 2: TAR@1%FAR\n",
        "def compute_tar_at_far(labels, scores, target_far=0.01):\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
        "    tar_at_far = interp1d(fpr, tpr)(target_far)\n",
        "    return tar_at_far * 100\n",
        "\n",
        "tar_at_1far = compute_tar_at_far(labels, scores, target_far=0.01)\n",
        "print(f\"TAR@1%FAR: {tar_at_1far:.2f}%\")"
      ],
      "metadata": {
        "id": "y9p1-1IkpYlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric 3: Speaker Identification Accuracy\n",
        "def compute_identification_accuracy(labels, scores, threshold=0.5):\n",
        "    predictions = [1 if score >= threshold else 0 for score in scores]\n",
        "    correct = sum(1 for pred, label in zip(predictions, labels) if pred == label)\n",
        "    accuracy = correct / len(labels) * 100  # Convert to percentage\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "xJPyyLh2pape"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use EER threshold for identification (optional: tune this)\n",
        "fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
        "eer_threshold = thresholds[np.argmin(np.abs(fpr - (1 - tpr)))]\n",
        "id_accuracy = compute_identification_accuracy(labels, scores, threshold=eer_threshold)\n",
        "print(f\"Speaker Identification Accuracy: {id_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "j00HshgCpc1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save scores and labels for further analysis\n",
        "np.save(\"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/scores.npy\", np.array(scores))\n",
        "np.save(\"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/labels.npy\", np.array(labels))"
      ],
      "metadata": {
        "id": "WtuPaOVipfLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "kHaiQ3rzphn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ArcFace Loss Implementation\n",
        "class ArcFaceLoss(nn.Module):\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
        "        super(ArcFaceLoss, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
        "        one_hot = torch.zeros_like(cosine).scatter_(1, labels.view(-1, 1), 1)\n",
        "        output = (one_hot * (theta + self.m) + (1.0 - one_hot) * theta).cos() * self.s\n",
        "        return F.cross_entropy(output, labels)\n",
        "\n",
        "# Custom Dataset with padding/truncation\n",
        "class VoxCeleb2Dataset(Dataset):\n",
        "    def __init__(self, files, max_length=48000):  # 3 seconds at 16kHz\n",
        "        self.files = files\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path, speaker_id = self.files[idx]\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "        if sample_rate != 16000:\n",
        "            waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
        "        waveform = waveform.squeeze(0)\n",
        "\n",
        "\n",
        "        if waveform.size(0) > self.max_length:\n",
        "            waveform = waveform[:self.max_length]\n",
        "        elif waveform.size(0) < self.max_length:\n",
        "            padding = torch.zeros(self.max_length - waveform.size(0))\n",
        "            waveform = torch.cat([waveform, padding])\n",
        "\n",
        "        return waveform, speaker_id"
      ],
      "metadata": {
        "id": "9OW1c6-1pkYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Load pre-trained model and feature extractor\n",
        "model_name = \"microsoft/wavlm-base-plus\"\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "model = WavLMModel.from_pretrained(model_name).to(device)\n"
      ],
      "metadata": {
        "id": "stboNieYpm_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=32,  # Increased rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"attention.q_proj\", \"attention.k_proj\", \"attention.v_proj\", \"attention.out_proj\"],\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.train()\n"
      ],
      "metadata": {
        "id": "LQXlSQPBppHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "voxceleb2_root = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/vox2/aac\"\n",
        "voxceleb1_trial_file = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/VoxCeleb1-cleaned.txt\"\n",
        "voxceleb1_root = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/wav\"\n"
      ],
      "metadata": {
        "id": "Okdfz-vZprYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VoxCeleb2 identities\n",
        "all_ids = sorted([d for d in os.listdir(voxceleb2_root) if os.path.isdir(os.path.join(voxceleb2_root, d))])[:118]\n",
        "train_ids = all_ids[:100]\n",
        "test_ids = all_ids[100:]\n",
        "\n",
        "# Prepare training data\n",
        "train_files = []\n",
        "for speaker_id in train_ids:\n",
        "    speaker_path = os.path.join(voxceleb2_root, speaker_id)\n",
        "    for session in os.listdir(speaker_path):\n",
        "        session_path = os.path.join(speaker_path, session)\n",
        "        files = [f for f in os.listdir(session_path) if f.endswith((\".wav\", \".m4a\"))]\n",
        "        for file in files:\n",
        "            train_files.append((os.path.join(session_path, file), speaker_id))\n",
        "\n",
        "print(f\"Collected {len(train_files)} training files from {len(train_ids)} speakers.\")\n",
        "\n",
        "# Fine-tuning setup\n",
        "train_dataset = VoxCeleb2Dataset(train_files[:5000])  # Larger subset\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "arcface_loss = ArcFaceLoss(in_features=768, out_features=len(train_ids)).to(device)\n",
        "id_to_idx = {id: idx for idx, id in enumerate(train_ids)}\n"
      ],
      "metadata": {
        "id": "MnnCmvGwpts2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for waveforms, speaker_ids in tqdm(train_loader):\n",
        "\n",
        "        inputs = feature_extractor(waveforms.tolist(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        input_values = inputs[\"input_values\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_values).last_hidden_state.mean(dim=1)\n",
        "        labels = torch.tensor([id_to_idx[sid] for sid in speaker_ids], dtype=torch.long).to(device)\n",
        "        loss = arcface_loss(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "_58HrAvXpvxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Evaluation function\n",
        "def extract_embedding(audio_path, model):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    if sample_rate != 16000:\n",
        "        waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
        "    waveform = waveform.squeeze(0)\n",
        "    inputs = feature_extractor(waveform.tolist(), sampling_rate=16e3, return_tensors=\"pt\", padding=True)\n",
        "    input_values = inputs[\"input_values\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_values)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    return embedding\n",
        "\n",
        "cosine_similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "\n",
        "# Load VoxCeleb1 trial pairs\n",
        "trials = []\n",
        "with open(voxceleb1_trial_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        label, file1, file2 = line.strip().split()\n",
        "        trials.append((int(label), file1, file2))\n"
      ],
      "metadata": {
        "id": "zBLIjLV2px7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate pre-trained and fine-tuned models\n",
        "def evaluate_model(model, name):\n",
        "    embedding_cache = {}\n",
        "    scores = []\n",
        "    labels = []\n",
        "    trial_subset = trials[:1000]\n",
        "    for label, file1, file2 in tqdm(trial_subset):\n",
        "        file1_path = os.path.join(voxceleb1_root, file1)\n",
        "        file2_path = os.path.join(voxceleb1_root, file2)\n",
        "\n",
        "        if not os.path.exists(file1_path) or not os.path.exists(file2_path):\n",
        "            print(f\"Skipping missing file: {file1_path} or {file2_path}\")\n",
        "            continue\n",
        "\n",
        "        if file1_path not in embedding_cache:\n",
        "            embedding_cache[file1_path] = extract_embedding(file1_path, model)\n",
        "        if file2_path not in embedding_cache:\n",
        "            embedding_cache[file2_path] = extract_embedding(file1_path, model)\n",
        "\n",
        "        emb1 = torch.from_numpy(embedding_cache[file1_path]).to(device)\n",
        "        emb2 = torch.from_numpy(embedding_cache[file2_path]).to(device)\n",
        "        score = cosine_similarity(emb1, emb2).item()\n",
        "        scores.append(score)\n",
        "        labels.append(label)\n",
        "\n",
        "    if not labels:\n",
        "        print(f\"No valid trial pairs processed for {name}. Check voxceleb1_root and trial file paths.\")\n",
        "        return None, None, None\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
        "    fnr = 1 - tpr\n",
        "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.) * 100\n",
        "    eer_threshold = thresholds[np.argmin(np.abs(fpr - (1 - tpr)))]\n",
        "    tar_at_1far = interp1d(fpr, tpr)(0.01) * 100\n",
        "    predictions = [1 if score >= eer_threshold else 0 for score in scores]\n",
        "    id_accuracy = sum(1 for pred, label in zip(predictions, labels) if pred == label) / len(labels) * 100\n",
        "\n",
        "    print(f\"{name} - EER: {eer:.2f}%, TAR@1%FAR: {tar_at_1far:.2f}%, Speaker ID Accuracy: {id_accuracy:.2f}%\")\n",
        "    return eer, tar_at_1far, id_accuracy\n",
        "\n",
        "# Load pre-trained model for comparison\n",
        "pretrained_model = WavLMModel.from_pretrained(model_name).to(device)\n",
        "pretrained_model.eval()\n",
        "\n",
        "# Evaluate both models\n",
        "pretrained_metrics = evaluate_model(pretrained_model, \"Pre-trained\")\n",
        "finetuned_metrics = evaluate_model(model, \"Fine-tuned\")"
      ],
      "metadata": {
        "id": "f5PjZ7hOp0fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IIIA"
      ],
      "metadata": {
        "id": "2dtgUD3Tp3-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "ctV5DPKfp5pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "voxceleb2_root = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/vox2/aac\"\n",
        "output_train_dir = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/output/train_mixtures\"\n",
        "output_test_dir = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/output/test_mixtures\"\n",
        "os.makedirs(output_train_dir, exist_ok=True)\n",
        "os.makedirs(output_test_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "hHAVw01lp8Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VoxCeleb2 identities (sorted ascending)\n",
        "all_ids = sorted([d for d in os.listdir(voxceleb2_root) if os.path.isdir(os.path.join(voxceleb2_root, d))])\n",
        "train_ids = all_ids[:50]  # First 50 for training\n",
        "test_ids = all_ids[50:100]  # Next 50 for testing\n",
        "\n",
        "# Function to load and resample audio\n",
        "def load_audio(file_path, target_sr=16000):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    if sample_rate != target_sr:\n",
        "        waveform = torchaudio.transforms.Resample(sample_rate, target_sr)(waveform)\n",
        "    return waveform.squeeze(0)\n",
        "# Function to mix two utterances\n",
        "def mix_utterances(file1, file2, max_length=48000):  # 3 seconds\n",
        "    wav1 = load_audio(file1)\n",
        "    wav2 = load_audio(file2)\n",
        "\n",
        "    # Truncate or pad to max_length\n",
        "    if wav1.size(0) > max_length:\n",
        "        wav1 = wav1[:max_length]\n",
        "    elif wav1.size(0) < max_length:\n",
        "        wav1 = torch.cat([wav1, torch.zeros(max_length - wav1.size(0))])\n",
        "\n",
        "    if wav2.size(0) > max_length:\n",
        "        wav2 = wav2[:max_length]\n",
        "    elif wav2.size(0) < max_length:\n",
        "        wav2 = torch.cat([wav2, torch.zeros(max_length - wav2.size(0))])\n",
        "\n",
        "    # Mix with random gain between 0.5 and 1.0\n",
        "    gain1, gain2 = random.uniform(0.5, 1.0), random.uniform(0.5, 1.0)\n",
        "    mixture = gain1 * wav1 + gain2 * wav2\n",
        "    mixture = mixture / torch.max(torch.abs(mixture))\n",
        "\n",
        "    return mixture, wav1, wav2"
      ],
      "metadata": {
        "id": "dg3S9dWNp-NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect files for each identity\n",
        "def collect_files(ids, root_dir):\n",
        "    files_dict = {}\n",
        "    for speaker_id in ids:\n",
        "        speaker_path = os.path.join(root_dir, speaker_id)\n",
        "        files = []\n",
        "        for session in os.listdir(speaker_path):\n",
        "            session_path = os.path.join(speaker_path, session)\n",
        "            files.extend([os.path.join(session_path, f) for f in os.listdir(session_path) if f.endswith(\".m4a\")])\n",
        "        files_dict[speaker_id] = files\n",
        "    return files_dict\n",
        "\n",
        "# Create mixtures\n",
        "def create_mixtures(ids, files_dict, output_dir, num_mixtures=100):\n",
        "    for i in tqdm(range(num_mixtures)):\n",
        "        # Randomly select two different speakers\n",
        "        spk1, spk2 = random.sample(ids, 2)\n",
        "        file1 = random.choice(files_dict[spk1])\n",
        "        file2 = random.choice(files_dict[spk2])\n",
        "\n",
        "        mixture, wav1, wav2 = mix_utterances(file1, file2)\n",
        "\n",
        "        # Save mixture and original sources\n",
        "        torchaudio.save(os.path.join(output_dir, f\"mix_{i}.wav\"), mixture.unsqueeze(0), 16000)\n",
        "        torchaudio.save(os.path.join(output_dir, f\"src1_{i}.wav\"), wav1.unsqueeze(0), 16000)\n",
        "        torchaudio.save(os.path.join(output_dir, f\"src2_{i}.wav\"), wav2.unsqueeze(0), 16000)\n"
      ],
      "metadata": {
        "id": "cXbmkItAqBNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate datasets\n",
        "train_files = collect_files(train_ids, voxceleb2_root)\n",
        "test_files = collect_files(test_ids, voxceleb2_root)\n",
        "create_mixtures(train_ids, train_files, output_train_dir, num_mixtures=100)  # 100 training mixtures\n",
        "create_mixtures(test_ids, test_files, output_test_dir, num_mixtures=50)    # 50 testing mixtures"
      ],
      "metadata": {
        "id": "QyTGKsaBqDzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "III 3 A"
      ],
      "metadata": {
        "id": "1rISCvAZqIt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from speechbrain.pretrained import SepformerSeparation\n",
        "from pesq import pesq\n",
        "from pystoi import stoi\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "e2yD3MPqqGGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Load pre-trained SepFormer model\n",
        "model = SepformerSeparation.from_hparams(\n",
        "    source=\"speechbrain/sepformer-wsj02mix\",\n",
        "    savedir=\"pretrained_models/sepformer-wsj02mix\"\n",
        ")\n",
        "\n",
        "# Evaluation metrics functions\n",
        "def compute_sdr(ref, est):\n",
        "    \"\"\"Simplified SDR calculation\"\"\"\n",
        "    s_target = ref\n",
        "    e_noise = est - ref\n",
        "    return 10 * np.log10(np.mean(s_target**2) / (np.mean(e_noise**2) + 1e-8))\n",
        "\n",
        "def compute_sir(ref, est, interferer):\n",
        "    \"\"\"Simplified SIR calculation\"\"\"\n",
        "    s_target = ref\n",
        "    e_interf = interferer\n",
        "    return 10 * np.log10(np.mean(s_target**2) / (np.mean(e_interf**2) + 1e-8))\n",
        "\n",
        "def compute_sar(ref, est):\n",
        "    \"\"\"Simplified SAR calculation\"\"\"\n",
        "    s_target = ref\n",
        "    e_artifacts = est - ref\n",
        "    return 10 * np.log10(np.mean(s_target**2) / (np.mean(e_artifacts**2) + 1e-8))\n"
      ],
      "metadata": {
        "id": "1LkFhnVOqMdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Paths\n",
        "test_dir = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/output/test_mixtures\"\n"
      ],
      "metadata": {
        "id": "966giE6VqPAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "results = {\"SIR\": [], \"SAR\": [], \"SDR\": [], \"PESQ\": []}\n",
        "for i in tqdm(range(50)):  # 50 test mixtures\n",
        "    mix_path = os.path.join(test_dir, f\"mix_{i}.wav\")\n",
        "    src1_path = os.path.join(test_dir, f\"src1_{i}.wav\")\n",
        "    src2_path = os.path.join(test_dir, f\"src2_{i}.wav\")\n",
        "\n",
        "    # Load mixture and references\n",
        "    mixture, sr = torchaudio.load(mix_path)\n",
        "    ref1, _ = torchaudio.load(src1_path)\n",
        "    ref2, _ = torchaudio.load(src2_path)\n",
        "    mixture = mixture.squeeze(0).numpy()\n",
        "    ref1 = ref1.squeeze(0).numpy()\n",
        "    ref2 = ref2.squeeze(0).numpy()\n",
        "\n",
        "    print(f\"Mixture length: {mixture.shape[0]} samples ({mixture.shape[0]/16000:.2f}s)\")\n",
        "\n",
        "    # Perform separation\n",
        "    est_sources = model.separate_file(mix_path)\n",
        "    est_sources = est_sources.squeeze(0).detach().cpu().numpy()\n",
        "    print(f\"Est sources shape: {est_sources.shape}\")\n",
        "\n",
        "    # Validate shape\n",
        "    if len(est_sources.shape) != 2 or est_sources.shape[1] != 2:\n",
        "        raise ValueError(f\"Expected [samples, 2], got {est_sources.shape}\")\n",
        "\n",
        "    est1, est2 = est_sources[:, 0], est_sources[:, 1]\n",
        "    print(f\"Est1 shape: {est1.shape}, Est2 shape: {est2.shape}\")\n",
        "\n",
        "    # Adjust lengths to match estimated sources\n",
        "    min_len = min(est1.shape[0], ref1.shape[0])\n",
        "    est1, est2 = est1[:min_len], est2[:min_len]\n",
        "    ref1, ref2 = ref1[:min_len], ref2[:min_len]\n",
        "    print(f\"Adjusted lengths to {min_len} samples ({min_len/16000:.2f}s)\")\n"
      ],
      "metadata": {
        "id": "vI8VfOJuqRrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Compute metrics\n",
        "    sir1 = compute_sir(ref1, est1, ref2)\n",
        "    sir2 = compute_sir(ref2, est2, ref1)\n",
        "    sar1 = compute_sar(ref1, est1)\n",
        "    sar2 = compute_sar(ref2, est2)\n",
        "    sdr1 = compute_sdr(ref1, est1)\n",
        "    sdr2 = compute_sdr(ref2, est2)\n",
        "    pesq1 = pesq(16000, ref1, est1, \"wb\")\n",
        "    pesq2 = pesq(16000, ref2, est2, \"wb\")\n",
        "\n",
        "    # Store results\n",
        "    results[\"SIR\"].extend([sir1, sir2])\n",
        "    results[\"SAR\"].extend([sar1, sar2])\n",
        "    results[\"SDR\"].extend([sdr1, sdr2])\n",
        "    results[\"PESQ\"].extend([pesq1, pesq2])\n",
        "\n",
        "# Compute averages\n",
        "for metric in results:\n",
        "    avg = np.mean(results[metric])\n",
        "    print(f\"Average {metric}: {avg:.2f}\")\n",
        "\n",
        "\"\"\"# Q. III B\"\"\"\n",
        "\n",
        "!pip install speechbrain\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2FeatureExtractor, WavLMModel\n",
        "from speechbrain.inference import SepformerSeparation\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained WavLM and feature extractor\n",
        "model_name = \"microsoft/wavlm-base-plus\"\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "pretrained_model = WavLMModel.from_pretrained(model_name).to(device)\n",
        "pretrained_model.eval()\n",
        "\n",
        "# Load fine-tuned WavLM (assuming saved from first task)\n",
        "finetuned_model = WavLMModel.from_pretrained(model_name).to(device)\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"attention.q_proj\", \"attention.k_proj\", \"attention.v_proj\", \"attention.out_proj\"],\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "\n",
        "finetuned_model = get_peft_model(finetuned_model, lora_config)\n",
        "# Load fine-tuned weights (update path to your saved model)\n",
        "finetuned_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/finetuned_model.pth\"))\n",
        "finetuned_model.eval()\n",
        "\n",
        "# Load SepFormer model\n",
        "sep_model = SepformerSeparation.from_hparams(\n",
        "    source=\"speechbrain/sepformer-wsj02mix\",\n",
        "    savedir=\"pretrained_models/sepformer-wsj02mix\"\n",
        ")"
      ],
      "metadata": {
        "id": "mxxzDRGzqUM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "test_dir = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/output/test_mixtures\"\n",
        "voxceleb2_root = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/vox2/aac\""
      ],
      "metadata": {
        "id": "8q0uK5iiqXZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test identities (50-99)\n",
        "all_ids = sorted([d for d in os.listdir(voxceleb2_root) if os.path.isdir(os.path.join(voxceleb2_root, d))])\n",
        "test_ids = all_ids[50:100]\n",
        "id_to_idx = {id: idx for idx, id in enumerate(test_ids)}\n",
        "\n",
        "# Function to extract embedding\n",
        "def extract_embedding(waveform, model):\n",
        "    inputs = feature_extractor(waveform.tolist(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    input_values = inputs[\"input_values\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_values)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    return embedding\n",
        "\n",
        "# Cosine similarity\n",
        "cosine_similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "\n",
        "# Collect reference embeddings for test identities\n",
        "ref_embeddings_pretrained = {}\n",
        "ref_embeddings_finetuned = {}\n",
        "for speaker_id in test_ids:\n",
        "    speaker_path = os.path.join(voxceleb2_root, speaker_id, os.listdir(os.path.join(voxceleb2_root, speaker_id))[0])\n",
        "    file = os.path.join(speaker_path, os.listdir(speaker_path)[0])\n",
        "    waveform, sr = torchaudio.load(file)\n",
        "    if sr != 16000:\n",
        "        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
        "    waveform = waveform.squeeze(0).numpy()\n",
        "\n",
        "    ref_embeddings_pretrained[speaker_id] = extract_embedding(waveform, pretrained_model)\n",
        "    ref_embeddings_finetuned[speaker_id] = extract_embedding(waveform, finetuned_model)\n",
        "\n",
        "# Evaluate on separated test set\n",
        "correct_pretrained = 0\n",
        "correct_finetuned = 0\n",
        "total = 0\n",
        "\n",
        "for i in tqdm(range(50)):  # 50 test mixtures\n",
        "    mix_path = os.path.join(test_dir, f\"mix_{i}.wav\")\n",
        "    src1_path = os.path.join(test_dir, f\"src1_{i}.wav\")\n",
        "    src2_path = os.path.join(test_dir, f\"src2_{i}.wav\")\n",
        "\n",
        "    # Ground truth speaker IDs\n",
        "    src1_file = os.path.basename(src1_path).split(\"_\")[1]\n",
        "    src2_file = os.path.basename(src2_path).split(\"_\")[1]\n",
        "    true_id1 = os.path.basename(os.path.dirname(os.path.dirname(src1_path)))\n",
        "    true_id2 = os.path.basename(os.path.dirname(os.path.dirname(src2_path)))\n",
        "\n",
        "    # Separation\n",
        "    est_sources = sep_model.separate_file(mix_path).squeeze(0).detach().cpu().numpy()\n",
        "    est1, est2 = est_sources[:, 0], est_sources[:, 1]\n",
        "\n",
        "    # Extract embeddings from separated sources\n",
        "    emb1_pretrained = extract_embedding(est1, pretrained_model)\n",
        "    emb2_pretrained = extract_embedding(est2, pretrained_model)\n",
        "    emb1_finetuned = extract_embedding(est1, finetuned_model)\n",
        "    emb2_finetuned = extract_embedding(est2, finetuned_model)\n",
        "\n",
        "    # Compute similarities and predict speakers\n",
        "    pretrained_scores = {}\n",
        "    finetuned_scores = {}\n",
        "    for speaker_id in test_ids:\n",
        "        ref_pre = torch.from_numpy(ref_embeddings_pretrained[speaker_id]).to(device)\n",
        "        ref_fin = torch.from_numpy(ref_embeddings_finetuned[speaker_id]).to(device)\n",
        "        pretrained_scores[speaker_id] = [\n",
        "            cosine_similarity(torch.from_numpy(emb1_pretrained).to(device), ref_pre).item(),\n",
        "            cosine_similarity(torch.from_numpy(emb2_pretrained).to(device), ref_pre).item()\n",
        "        ]\n",
        "        finetuned_scores[speaker_id] = [\n",
        "            cosine_similarity(torch.from_numpy(emb1_finetuned).to(device), ref_fin).item(),\n",
        "            cosine_similarity(torch.from_numpy(emb2_finetuned).to(device), ref_fin).item()\n",
        "        ]\n",
        "\n",
        "    # Rank-1 prediction\n",
        "    pred_id1_pre = max(pretrained_scores, key=lambda k: pretrained_scores[k][0])\n",
        "    pred_id2_pre = max(pretrained_scores, key=lambda k: pretrained_scores[k][1])\n",
        "    pred_id1_fin = max(finetuned_scores, key=lambda k: finetuned_scores[k][0])\n",
        "    pred_id2_fin = max(finetuned_scores, key=lambda k: finetuned_scores[k][1])\n",
        "\n",
        "    # Check correctness (permutation invariant)\n",
        "    pre_correct = (pred_id1_pre == true_id1 and pred_id2_pre == true_id2) or \\\n",
        "                  (pred_id1_pre == true_id2 and pred_id2_pre == true_id1)\n",
        "    fin_correct = (pred_id1_fin == true_id1 and pred_id2_fin == true_id2) or \\\n",
        "                  (pred_id1_fin == true_id2 and pred_id2_fin == true_id1)\n",
        "\n",
        "    correct_pretrained += pre_correct\n",
        "    correct_finetuned += fin_correct\n",
        "    total += 1\n",
        "\n",
        "# Compute Rank-1 accuracy\n",
        "rank1_acc_pretrained = correct_pretrained / total * 100\n",
        "rank1_acc_finetuned = correct_finetuned / total * 100\n",
        "\n",
        "print(f\"Pre-trained WavLM Rank-1 Accuracy: {rank1_acc_pretrained:.2f}%\")\n",
        "print(f\"Fine-tuned WavLM Rank-1 Accuracy: {rank1_acc_finetuned:.2f}%\")\n"
      ],
      "metadata": {
        "id": "dQzUFzl1qcyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q. IV A,B"
      ],
      "metadata": {
        "id": "GbsUl8_SqfVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pesq\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2FeatureExtractor, WavLMModel\n",
        "from speechbrain.pretrained import SepformerSeparation\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pesq import pesq"
      ],
      "metadata": {
        "id": "Wi6yjzhxqdhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "voxceleb2_root = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/vox2/aac\"\n",
        "train_dir = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/output/train_mixtures\"\n",
        "test_dir = \"/content/drive/MyDrive/Colab Notebooks/SEM03-Assignments/Speech Understanding/Assignment2/output/test_mixtures\""
      ],
      "metadata": {
        "id": "THHNifzoqiod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Load models\n",
        "model_name = \"microsoft/wavlm-base-plus\"\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "pretrained_wavlm = WavLMModel.from_pretrained(model_name).to(device)\n",
        "pretrained_wavlm.eval()\n",
        "\n",
        "# Fine-tuned WavLM with LoRA\n",
        "finetuned_wavlm = WavLMModel.from_pretrained(model_name).to(device)\n",
        "lora_config = LoraConfig(r=32, lora_alpha=32, target_modules=[\"attention.q_proj\", \"attention.k_proj\", \"attention.v_proj\", \"attention.out_proj\"], lora_dropout=0.1)\n",
        "finetuned_wavlm = get_peft_model(finetuned_wavlm, lora_config)\n",
        "\n",
        "# SepFormer\n",
        "sepformer = SepformerSeparation.from_hparams(source=\"speechbrain/sepformer-wsj02mix\", savedir=\"pretrained_models/sepformer-wsj02mix\").to(device)\n",
        "\n",
        "# Dataset\n",
        "class MultiSpeakerDataset(Dataset):\n",
        "    def __init__(self, data_dir, max_length=48000):\n",
        "        self.data_dir = data_dir\n",
        "        self.max_length = max_length\n",
        "        self.files = [f for f in os.listdir(data_dir) if f.startswith(\"mix_\") and f.endswith(\".wav\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mix_path = os.path.join(self.data_dir, self.files[idx])\n",
        "        src1_path = os.path.join(self.data_dir, f\"src1_{idx}.wav\")\n",
        "        src2_path = os.path.join(self.data_dir, f\"src2_{idx}.wav\")\n",
        "\n",
        "        mix, sr = torchaudio.load(mix_path)\n",
        "        src1, _ = torchaudio.load(src1_path)\n",
        "        src2, _ = torchaudio.load(src2_path)\n",
        "\n",
        "        if sr != 16000:\n",
        "            mix = torchaudio.transforms.Resample(sr, 16000)(mix)\n",
        "            src1 = torchaudio.transforms.Resample(sr, 16000)(src1)\n",
        "            src2 = torchaudio.transforms.Resample(sr, 16000)(src2)\n",
        "\n",
        "        mix, src1, src2 = mix.squeeze(0), src1.squeeze(0), src2.squeeze(0)\n",
        "        if mix.size(0) > self.max_length:\n",
        "            mix, src1, src2 = mix[:self.max_length], src1[:self.max_length], src2[:self.max_length]\n",
        "        elif mix.size(0) < self.max_length:\n",
        "            padding = torch.zeros(self.max_length - mix.size(0))\n",
        "            mix = torch.cat([mix, padding])\n",
        "            src1 = torch.cat([src1, padding])\n",
        "            src2 = torch.cat([src2, padding])\n",
        "\n",
        "        # Extract IDs from filenames (assuming format src1_idXXXXX_idx.wav)\n",
        "        id1 = src1_path.split(\"src1_\")[1].split(\"_\")[0] if \"src1_\" in src1_path else os.path.basename(os.path.dirname(os.path.dirname(src1_path)))\n",
        "        id2 = src2_path.split(\"src2_\")[1].split(\"_\")[0] if \"src2_\" in src2_path else os.path.basename(os.path.dirname(os.path.dirname(src2_path)))\n",
        "        return mix, src1, src2, id1, id2\n"
      ],
      "metadata": {
        "id": "1rNOTt03qlKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_dataset = MultiSpeakerDataset(train_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_dataset = MultiSpeakerDataset(test_dir)\n",
        "\n",
        "# Identification loss\n",
        "class ArcFaceLoss(nn.Module):\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
        "        super(ArcFaceLoss, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
        "        one_hot = torch.zeros_like(cosine).scatter_(1, labels.view(-1, 1), 1)\n",
        "        output = (one_hot * (theta + self.m) + (1.0 - one_hot) * theta).cos() * self.s\n",
        "        return F.cross_entropy(output, labels)\n",
        "\n",
        "# Training setup\n",
        "train_ids = sorted([d for d in os.listdir(voxceleb2_root) if os.path.isdir(os.path.join(voxceleb2_root, d))])[:50]\n",
        "test_ids = sorted([d for d in os.listdir(voxceleb2_root) if os.path.isdir(os.path.join(voxceleb2_root, d))])[50:100]\n",
        "id_to_idx = {id: idx for idx, id in enumerate(train_ids)}\n",
        "optimizer = torch.optim.Adam(list(sepformer.parameters()) + list(finetuned_wavlm.parameters()), lr=1e-4)\n",
        "arcface_loss = ArcFaceLoss(in_features=768, out_features=len(train_ids)).to(device)\n",
        "cosine_similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "\n",
        "# Fine-tuning loop\n",
        "def train_pipeline():\n",
        "    sepformer.train()\n",
        "    finetuned_wavlm.train()\n",
        "    for epoch in range(5):\n",
        "        total_loss = 0\n",
        "        for mix, src1, src2, id1, id2 in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            mix, src1, src2 = mix.to(device), src1.to(device), src2.to(device)\n",
        "            print(f\"ID1: {id1}, ID2: {id2}\")  # Debug\n",
        "            labels = torch.tensor([id_to_idx[i] for i in id1] + [id_to_idx[i] for i in id2], dtype=torch.long).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            est_sources = sepformer(mix.unsqueeze(1))  # [batch, samples, 2]\n",
        "            print(f\"SepFormer output shape: {est_sources.shape}\")\n",
        "            est1, est2 = est_sources[..., 0], est_sources[..., 1]\n",
        "\n",
        "            inputs1 = feature_extractor(est1.tolist(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "            inputs2 = feature_extractor(est2.tolist(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "            emb1 = finetuned_wavlm(inputs1[\"input_values\"].to(device)).last_hidden_state.mean(dim=1)\n",
        "            emb2 = finetuned_wavlm(inputs2[\"input_values\"].to(device)).last_hidden_state.mean(dim=1)\n",
        "            embeddings = torch.cat([emb1, emb2], dim=0)\n",
        "\n",
        "            sep_loss = -torch.mean(torch.tensor([compute_sdr(src1[i].cpu().numpy(), est1[i].cpu().numpy()) +\n",
        "                                                compute_sdr(src2[i].cpu().numpy(), est2[i].cpu().numpy())\n",
        "                                                for i in range(mix.size(0))], requires_grad=True).to(device))\n",
        "            id_loss = arcface_loss(embeddings, labels)\n",
        "            loss = sep_loss + 0.1 * id_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Metric functions\n",
        "def compute_sdr(ref, est):\n",
        "    s_target = ref\n",
        "    e_noise = est - ref\n",
        "    return 10 * np.log10(np.mean(s_target**2) / (np.mean(e_noise**2) + 1e-8))\n",
        "\n",
        "def compute_sir(ref, est, interferer):\n",
        "    s_target = ref\n",
        "    e_interf = interferer\n",
        "    return 10 * np.log10(np.mean(s_target**2) / (np.mean(e_interf**2) + 1e-8))\n",
        "\n",
        "def compute_sar(ref, est):\n",
        "    s_target = ref\n",
        "    e_artifacts = est - ref\n",
        "    return 10 * np.log10(np.mean(s_target**2) / (np.mean(e_artifacts**2) + 1e-8))\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_pipeline():\n",
        "    sepformer.eval()\n",
        "    pretrained_wavlm.eval()\n",
        "    finetuned_wavlm.eval()\n",
        "    results = {\"SIR\": [], \"SAR\": [], \"SDR\": [], \"PESQ\": []}\n",
        "    correct_pre, correct_fin, total = 0, 0, 0\n",
        "\n",
        "    ref_emb_pre, ref_emb_fin = {}, {}\n",
        "    for speaker_id in test_ids:\n",
        "        speaker_path = os.path.join(voxceleb2_root, speaker_id, os.listdir(os.path.join(voxceleb2_root, speaker_id))[0])\n",
        "        file = os.path.join(speaker_path, os.listdir(speaker_path)[0])\n",
        "        waveform, sr = torchaudio.load(file)\n",
        "        if sr != 16000:\n",
        "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
        "        waveform = waveform.squeeze(0).numpy()\n",
        "        ref_emb_pre[speaker_id] = extract_embedding(waveform, pretrained_wavlm)\n",
        "        ref_emb_fin[speaker_id] = extract_embedding(waveform, finetuned_wavlm)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(test_dataset)), desc=\"Evaluating\"):\n",
        "            mix, src1, src2, id1, id2 = test_dataset[i]\n",
        "            mix = mix.unsqueeze(0).to(device)\n",
        "            src1, src2 = src1.numpy(), src2.numpy()\n",
        "\n",
        "            est_sources = sepformer(mix.unsqueeze(1)).squeeze(0).cpu().numpy()\n",
        "            est1, est2 = est_sources[:, 0], est_sources[:, 1]\n",
        "\n",
        "            min_len = min(est1.shape[0], src1.shape[0])\n",
        "            est1, est2 = est1[:min_len], est2[:min_len]\n",
        "            src1, src2 = src1[:min_len], src2[:min_len]\n",
        "\n",
        "            results[\"SIR\"].extend([compute_sir(src1, est1, src2), compute_sir(src2, est2, src1)])\n",
        "            results[\"SAR\"].extend([compute_sar(src1, est1), compute_sar(src2, est2)])\n",
        "            results[\"SDR\"].extend([compute_sdr(src1, est1), compute_sdr(src2, est2)])\n",
        "            results[\"PESQ\"].extend([pesq(16000, src1, est1, \"wb\"), pesq(16000, src2, est2, \"wb\")])\n",
        "\n",
        "            emb1_pre = extract_embedding(est1, pretrained_wavlm)\n",
        "            emb2_pre = extract_embedding(est2, pretrained_wavlm)\n",
        "            emb1_fin = extract_embedding(est1, finetuned_wavlm)\n",
        "            emb2_fin = extract_embedding(est2, finetuned_wavlm)\n",
        "\n",
        "            pre_scores, fin_scores = {}, {}\n",
        "            for sid in test_ids:\n",
        "                ref_pre = torch.from_numpy(ref_emb_pre[sid]).to(device)\n",
        "                ref_fin = torch.from_numpy(ref_emb_fin[sid]).to(device)\n",
        "                pre_scores[sid] = [cosine_similarity(torch.from_numpy(emb1_pre).to(device), ref_pre).item(),\n",
        "                                   cosine_similarity(torch.from_numpy(emb2_pre).to(device), ref_pre).item()]\n",
        "                fin_scores[sid] = [cosine_similarity(torch.from_numpy(emb1_fin).to(device), ref_fin).item(),\n",
        "                                   cosine_similarity(torch.from_numpy(emb2_fin).to(device), ref_fin).item()]\n",
        "\n",
        "            pred_id1_pre = max(pre_scores, key=lambda k: pre_scores[k][0])\n",
        "            pred_id2_pre = max(pre_scores, key=lambda k: pre_scores[k][1])\n",
        "            pred_id1_fin = max(fin_scores, key=lambda k: fin_scores[k][0])\n",
        "            pred_id2_fin = max(fin_scores, key=lambda k: fin_scores[k][1])\n",
        "\n",
        "            pre_correct = (pred_id1_pre == id1 and pred_id2_pre == id2) or (pred_id1_pre == id2 and pred_id2_pre == id1)\n",
        "            fin_correct = (pred_id1_fin == id1 and pred_id2_fin == id2) or (pred_id1_fin == id2 and pred_id2_fin == id1)\n",
        "            correct_pre += pre_correct\n",
        "            correct_fin += fin_correct\n",
        "            total += 1\n",
        "\n",
        "    for metric in results:\n",
        "        avg = np.mean(results[metric])\n",
        "        print(f\"Average {metric}: {avg:.2f}\")\n",
        "    rank1_pre = correct_pre / total * 100\n",
        "    rank1_fin = correct_fin / total * 100\n",
        "    print(f\"Pre-trained WavLM Rank-1 Accuracy: {rank1_pre:.2f}%\")\n",
        "    print(f\"Fine-tuned WavLM Rank-1 Accuracy: {rank1_fin:.2f}%\")\n",
        "\n",
        "# Extract embedding\n",
        "def extract_embedding(waveform, model):\n",
        "    inputs = feature_extractor(waveform.tolist(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    input_values = inputs[\"input_values\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_values)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    return embedding\n",
        "\n",
        "# Run pipeline\n",
        "print(\"Training SepID-Enhance Pipeline...\")\n",
        "train_pipeline()\n",
        "print(\"\\nEvaluating on Test Set...\")\n",
        "evaluate_pipeline()\n"
      ],
      "metadata": {
        "id": "ztXKf12Wqnkl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}